{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerneral Rules\n",
    "\n",
    "1. A testing unit should focus on one tiny bit of functionality and prove it correct\n",
    "\n",
    "2. Each test unit must be fully independent. Each test must be able to run ablone, and also within the test suit, regardless of the order that thy are called. Each test must be loaded with a fresh dataset and may have to do some cleanup afterwards. This is usually handled by `setUp()` and `tearDown()` methods\n",
    "\n",
    "3. Make tests that run fast and keep heavier tests in a seperate test suit that is run by some scheduled teask, and run all other tests as often as needed\n",
    "\n",
    "4. Always run the full test suit before coding session, and run it again after. \n",
    "\n",
    "5. Implement a hook that runs all tests before pushing code to a shared repository.\n",
    "\n",
    "6. If development session is interrupted, write down a broken unit test about what you want to develop next. When coming back to work, you will have a pointer to where you were and get back on track faster.\n",
    "\n",
    "7. The first step when you are debugging is to write a new test pinpointing the bug. \n",
    "\n",
    "8. Use long and descriptive names for testing functions. However, Sshort names are always preferred for when running code. \n",
    "\n",
    "9. The testing code wil be read as much as or even more than the running ocde. A unit test whose purpose is unclear is not very helpful in this case. \n",
    "\n",
    "10. Another use of the testing code is as an introduction to new developers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basics\n",
    "\n",
    "### unittest\n",
    "\n",
    "`unittest` is the batteries-inlcuded test module in the python standard library. \n",
    "\n",
    "Creating test cases is accomplished by subclassing `unittest.TestCase`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "def fun(x):\n",
    "    return x+1\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "    def test(self):\n",
    "        self.assetEqual(fun(3),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doctest \n",
    "\n",
    "The doctest module searches for pieces of text that looks like interactive detailed and don't catch special cases or obscure regression bugs (A regression bug is a bug which causes a feature that worked correctly to stop working after a certain event, normally caused by deep nested tests). They are useful as an expressive documentation of the main use cases of a module and its components. However, doctests should run automatically each time the full test suite runs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    \"\"\"Return the square of x.\n",
    "    \n",
    "    >>> square(2)\n",
    "    4\n",
    "    >>> square(-2)\n",
    "    4\n",
    "    \"\"\"\n",
    "    \n",
    "    return x * x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import doctest\n",
    "    doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running this module from the command line as in python module.py, the doctests will run and complain if anything is not behaving as described in the docstrings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "### py.test\n",
    "py.test is a no-boilerplate alternative to Python's standard unittest module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content of test_sample.py\n",
    "def func(x):\n",
    "    return x+1\n",
    "\n",
    "def test_answer():\n",
    "    assert func(3) == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then running the py.test command:\n",
    "\n",
    "`\n",
    "$ py.test\n",
    "`\n",
    "\n",
    "It's far less work than would be required for the equivalent functionality with the unittest module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis \n",
    "\n",
    "Hypothesis is a library which lets you write tests that are parameterized by a source of examples. It then generates simple and comprehensible examples that make your test fail, letting you find more bugs with less work. \n",
    "\n",
    "https://hypothesis.readthedocs.io/en/latest/\n",
    "\n",
    "For example, testing lists of floats will try many ecamples, but report the minimal example of each bug (distinguished exception type and location):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @given(lists(floats(allow_nan=False, allow_infinity=False), min_size = 1))\n",
    "def test_mean(xs):\n",
    "    mean = sum(xs) / len(xs)\n",
    "    assert min(xs) <= mean(xs) <= max(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a9637a369cd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m test_mean(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.7976321109618856e+308\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.102390043022755e+303\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-10-d2d97ca71f60>\u001b[0m in \u001b[0;36mtest_mean\u001b[0;34m(xs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "test_mean(\n",
    "    xs=[1.7976321109618856e+308, 6.102390043022755e+303]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
